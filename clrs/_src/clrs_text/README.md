# The CLRS-Text Algorithmic Reasoning Language Benchmark

Eliciting reasoning capabilities from language models (LMs) is a critical
direction on the path towards building intelligent systems. Most recent studies dedicated to reasoning focus on out-of-distribution performance on
procedurally-generated synthetic benchmarks, bespoke-built to evaluate specific
skills only. One standard way in which OOD performance is measured, for example,
is by assessing [length generalisation](https://arxiv.org/abs/2402.09371).

CLRS-Text is a textual version of the traces generated by thirty
algorithms selected from the third edition of the standard
*Introduction to Algorithms* textbook by Cormen, Leiserson, Rivest and Stein. It
serves to consolidate and unify previous lines of research in this direction,
and offer a robust test-bed for evaluating language models' out-of-distribution
reasoning capabilities.

## How it works

CLRS-Text is designed as a text-based wrapper around the traces produced by the
base version of the CLRS benchmark. Therefore, all the key details on how the
data is generated and how it may be extended to additional distributions and/or
algorithms are identical to the details for CLRS---please consult the
[base README file](https://github.com/google-deepmind/clrs/blob/master/README.md)
for further information.

## What we offer

What we do provide on top of CLRS are the following libraries:
* `clrs_utils.py`: base functionalities for converting a CLRS trace into
strings. Note that currently the convertors cannot manage padding, and as a
result it is recommended to always process traces sampled with a batch size of 1
and without max-hint tracking.
* `generate_clrs_text.py`: pre-packaged scripts that can generate the entirety
of the CLRS-Text training and eval sets with the statistics advertised in our
paper's evaluation. The outputs are in JSON format. Please see the docstring of
the script for more details about how to launch it.
* `huggingface_generators.py`: a convenience function that abstracts away all
the underlying sampler and generation calls to CLRS and CLRS-Text and provides
Hugging Face-compatible samples out-of-the-box given a few hyperparameters.

For convenience, we have created a [Hugging Face Collection](https://huggingface.co/collections/tomg-group-umd/clrs-text-668ea13343dab55d6efa1a30)
which comprises exactly the examples used to train and evaluate our models
in the original CLRS-Text paper's evaluation.

## Citation

To cite the CLRS-Text Algorithmic Reasoning Language Benchmark:

```latex
@article{deepmind2024clrstext,
  title={The CLRS-Text Algorithmic Reasoning Language Benchmark},
  author={Larisa Markeeva and Sean McLeish and Borja Ibarz and Wilfried Bounsi
    and Olga Kozlova and Alex Vitvitskyi and Charles Blundell and
    Tom Goldstein and Avi Schwarzschild and Petar Veli\v{c}kovi\'{c}},
  journal={arXiv preprint arXiv:2406.04229},
  year={2024}
}
```
